# -*- coding: utf-8 -*-
"""Exploratory Data Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Ga_J_aUJ6Od4Dksm0hk-PyxLVYceoRd

# Exploratory Data Analysis
"""

# Import Dependencies

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

import warnings

warnings.filterwarnings('ignore')

pd.set_option('display.max_rows', None)
display_set = pd.read_csv('/content/EX008.csv')

display_set.head(10)

display_set.describe()

"""# Creating a dataframe from all the files"""

# Define a function to process each file
def process_file(file_path):
  # Read the CSV files
  df = pd.read_csv(file_path, dtype={'Description': 'object', 'Value': 'object', 'Timing': 'object'})

  # Extract ABI values
  ll_abi = df.loc[df['Description'] == 'LABI', 'Value'].iloc[0]
  rl_abi = df.loc[df['Description'] == 'RABI', 'Value'].iloc[0]

  # Convert ABI values to numeric and round off to two decimal places
  ll_abi = round(float(ll_abi), 2)
  rl_abi = round(float(rl_abi), 2)

  # Create new column names
  new_rl_col = 'RL_PVR_' + str(rl_abi)
  new_ll_col = 'LL_PVR_' + str(ll_abi)

  # Create a new DataFrame with renamed columns
  new_df = pd.DataFrame({
      new_rl_col: df['RL_PVR'],
      new_ll_col: df['LL_PVR']
  })

  # Fill missing values with 0 and round off to two decimal places
  new_df = new_df.fillna(0).round(2)

  return new_df

# Process the data from all files
all_dataframes = []
for i in range(1, 9):
  file_name = f'EX00{i}.csv'
  processed_df = process_file(file_name)
  all_dataframes.append(processed_df)

# Concatenate all DataFrames
test_df = pd.concat(all_dataframes, axis=1)

# Display the first 20 rows of the Dataframe
print(test_df.head(20).to_markdown(index=False, numalign="left", stralign="left"))

test_data = test_df.head(400)
test_data.to_csv('/content/test_data.csv', index=False)

# Define a function to process each file
def process_file(file_path):
  # Read the CSV file with specified dtypes
  df = pd.read_csv(file_path, dtype={'Description': 'object', 'Value': 'object', 'Timing': 'object'})

  # Check if 'LABI' or 'RABI' is missing
  if 'LABI' not in df['Description'].values:
      print(f"'LABI' value is missing in {file_path}")
      ll_abi = pd.NA
      ll_category = 'Missing'
  else:
      ll_abi = float(df.loc[df['Description'] == 'LABI', 'Value'].iloc[0])
      ll_abi = round(ll_abi, 2)

      if ll_abi >= 0.9:
          ll_category = 'Control'
      elif 0.9 > ll_abi >= 0.8:
          ll_category = 'Mild'
      elif 0.8 > ll_abi >= 0.65:
          ll_category = 'Moderate'
      elif ll_abi < 0.65:
          ll_category = 'Critical'
      else:
          ll_category = 'Unidentified'

  if 'RABI' not in df['Description'].values:
      print(f"'RABI' value is missing in {file_path}")
      rl_abi = pd.NA
      rl_category = 'Missing'
  else:
      rl_abi = float(df.loc[df['Description'] == 'RABI', 'Value'].iloc[0])
      rl_abi = round(rl_abi, 2)

      if rl_abi >= 0.9:
          rl_category = 'Control'
      elif 0.9 > rl_abi >= 0.8:
          rl_category = 'Mild'
      elif 0.8 > rl_abi >= 0.65:
          rl_category = 'Moderate'
      elif rl_abi < 0.65:
          rl_category = 'Critical'
      else:
          rl_category = 'Unidentified'

  # Create Series for right and left leg data
  rl_series = pd.Series(df['RL_PVR'].fillna(0).round(2), name=rl_category)
  ll_series = pd.Series(df['LL_PVR'].fillna(0).round(2), name=ll_category)

  # Create a new DataFrame with both columns
  new_df = pd.concat([rl_series, ll_series], axis=1)

  return new_df

# Process files EX001.csv to EX008.csv
all_dataframes = []
for i in range(1, 9):
  file_name = f'EX00{i}.csv'
  processed_df = process_file(file_name)
  all_dataframes.append(processed_df)

# Concatenate all DataFrames
final_df = pd.concat(all_dataframes, axis=1)

# Display the first 10 rows of the final DataFrame
print(final_df.head(10).to_markdown(index=False, numalign="left", stralign="left"))

# Create a new dataframe `truncated_df` by selecting the first 400 rows of `final_df`
train_df = final_df.head(400)

# Print the first 5 rows of the `train_df` dataframe
print(train_df.head().to_markdown( numalign="left", stralign="left"))

final_df.to_csv('/content/train_data.csv', index=False)

test_data = pd.read_csv('/content/test_data.csv')
test_data.head()

# Create a figure with 16 subplots arranged in a 4x4 grid
fig, axes = plt.subplots(4, 4, figsize=(15, 10))

# Iterate through each column of the final_df dataframe
for i, (column_name, column_data) in enumerate(train_df.items()):
    # Access the corresponding subplot using the index
    row = i // 4
    col = i % 4
    ax = axes[row, col]

    # Determine the color based on the column name (category)
    color = 'red' if column_name == 'Critical' else 'grey'

    # Plot a line chart with the index on the x-axis and the column values on the y-axis
    ax.plot(column_data.index, column_data.values, color=color)

    # Add a title to the subplot indicating the column name (category)
    ax.set_title(column_name)

# Adjust the layout to prevent overlapping elements
plt.tight_layout()

# Display the plot
plt.show()

# Create a figure with 4 subplots arranged in a 2x2 grid
fig, axes = plt.subplots(2, 2, figsize=(10, 8))

# Flatten the axes array for easier indexing
axes = axes.flatten()

# Iterate through the columns of the plot_df dataframe along with their indices, ensuring the loop runs only 4 times
for i, (column_name, column_data) in enumerate(train_df.items()):
    if i >= 4:  # Stop the loop after processing 4 columns
        break

    # Access the corresponding subplot using the index
    ax = axes[i]

    # Determine the color based on the column name (category)
    color = 'red' if column_name == 'Critical' else 'grey'

    # Plot a line chart with the index on the x-axis and the column values on the y-axis, setting the color
    ax.plot(column_data.index, column_data.values, color=color)

    # Add a title to the subplot indicating the column name (category)
    ax.set_title(column_name)

# Adjust the layout to prevent overlapping elements
plt.tight_layout()

# Display the plot
plt.show()

"""# Feature Extraction (Statsmodels)"""

from scipy.stats import describe

# Extract the first column's name (ABI score) and its values
Label = train_df.columns[0]
pvr_signal = train_df.iloc[:, 0]

# Compute descriptive statistics
stats_summary = describe(pvr_signal)

# Create a Series from the stats summary and add ABI score
extracted_features_statsmodels = pd.Series(stats_summary)
extracted_features_statsmodels['Label'] = Label

# Display the extracted features
print("Extracted Features using Statsmodels for the first person:\n")
print(extracted_features_statsmodels.to_markdown(numalign="left", stralign="left"))

all_extracted_features = []

# Iterate over each column in the DataFrame
for col in train_df.columns:

    abi_score = col
    pvr_signal = train_df[col]

    # Compute descriptive statistics
    stats_summary = describe(pvr_signal)

    # Create a Series from the stats summary and add ABI score
    extracted_features_series = pd.Series(stats_summary)
    extracted_features_series['ABI_score'] = abi_score

    # Append the Series to the list
    all_extracted_features.append(extracted_features_series)

# Create a DataFrame from the list of extracted features
extracted_features_df = pd.DataFrame(all_extracted_features)
extracted_features_df.columns = ['count', 'min_max', 'mean', 'variance', 'skewness', 'kurtosis', 'Label']

# Display the first 5 rows of the extracted features DataFrame
print("Extracted Features using Statsmodels for all persons:\n")
print(extracted_features_df.head().to_markdown(numalign="left", stralign="left"))

from scipy.stats import describe

# Initialize a list to store results for each column
all_columns_stats = []

# Iterate over all columns
for col_idx in range(train_df.shape[1]):

    # Extract column name and values
    col_name = train_df.columns[col_idx]
    col_values = train_df.iloc[:, col_idx]

    # Compute descriptive statistics
    stats_summary = describe(col_values)

    # Create a Series with the appropriate column names
    col_stats = pd.Series(stats_summary, index=['count', 'minmax', 'mean', 'variance', 'skewness', 'kurtosis'])
    col_stats['Label'] = col_name

    # Append to the list
    all_columns_stats.append(col_stats)

# Concatenate all results into a DataFrame
final_stats_df = pd.concat(all_columns_stats, axis=1).T

# Display the extracted features for all columns
print("Extracted Features using Statsmodels for all columns:\n")
print(final_stats_df.to_markdown(numalign="left", stralign="left"))

# Split minmax into two separate columns
final_stats_df[['min', 'max']] = pd.DataFrame(final_stats_df['minmax'].tolist(), index=final_stats_df.index)

# Drop the original 'minmax' column
training_set = final_stats_df.drop(['minmax', 'count'], axis=1)

training_set.head(10)

training_set.to_csv('/content/evaluation_set.csv', index=False)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Drop the target feature from the dataset for training
X = training_set.drop('Label', axis=1)
y = training_set['Label']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Random Forest model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Classification report
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC

# Create an SVM classifier
svm_model = SVC()

# Train the model
svm_model.fit(X_train, y_train)

# Make predictions
y_pred_svm = svm_model.predict(X_test)

# Example for classification:
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.neural_network import MLPClassifier
from sklearn.utils import class_weight

# Calculate class weights (assuming 'y' contains your target labels)
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)
class_weights_dict = dict(enumerate(class_weights))

# Create an ANN classifier
ann_model = MLPClassifier(hidden_layer_sizes=(100,),
                          activation='relu',
                          solver='adam',
                          max_iter=200)

# Train the model with class weights
ann_model.fit(X_train, y_train)

# Example for classification:
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# Noise Addition to Boost the Sample Dataset"""

# Augmenting the dataset with noise

def add_noise(X, noise_factor=0.05):
    noise = np.random.normal(0, noise_factor, X.shape)
    return X + noise

X_train_augmented = add_noise(X_train)

print("Unique classes in y_train:", np.unique(y_train))
print("Keys in class_weights_dict:", class_weights_dict.keys())

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Drop the target feature from the dataset for training
X = training_set.drop('Label', axis=1)
y = training_set['Label']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add noise ti the training set
def add_noise(X, noise_factor=0.05):
    noise = np.random.normal(0, noise_factor, X.shape)
    return X + noise

# Augment the training data
X_train_augmented = add_noise(X_train.values)

# Create and train the Random Forest model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Classification report
from sklearn.metrics import accuracy_score, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.utils import class_weight

# Drop the target feature from the dataset for training
X = training_set.drop('Label', axis=1)
y = training_set['Label']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Function to add noise to numerical features
def add_noise(X, noise_factor=0.05):
    noise = np.random.normal(0, noise_factor, X.shape)
    return X + noise

# Augment the training data
X_train_augmented = add_noise(X_train.values)

# Calculate class weights
class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)
class_weights_dict = dict(enumerate(class_weights))

# Get unique classes and their corresponding numerical labels from y_train
classes = np.unique(y_train)

# Calculate class weights
class_weights = class_weight.compute_class_weight('balanced', classes=classes, y=y_train)

# Create a dictionary mapping class names to their weights
class_weights_dict = dict(zip(classes, class_weights))

# Create a Random Forest classifier with class weights
rf_model = RandomForestClassifier(class_weight=class_weights_dict)

# Train the model with augmented data
rf_model.fit(X_train_augmented, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# Signal Normalization"""

final_df.head(10)

"""# Generating Synthetic PVR Waveforms"""

import numpy as np
import matplotlib.pyplot as plt

def generate_pvr_with_pad(severity, random_state=None):
    if random_state is not None:
        np.random.seed(random_state)

    # Define Parameters (with variability)
    duration = 5 + np.random.normal(0, 0.2)  # Add variability to duration
    sampling_rate = 1000
    time = np.linspace(0, duration, int(duration * sampling_rate))

    # Baseline Pulse Shape (with variability)
    systolic_peak = 1.0 + np.random.normal(0, 0.1)
    diastolic_baseline = 0.1 + np.random.normal(0, 0.05)
    rise_time = 0.1 + np.random.normal(0, 0.02)
    decay_time = 0.3 + np.random.normal(0, 0.05)
    pulse = np.zeros_like(time)
    systolic_period = np.logical_and(time >= 0, time <= rise_time)
    pulse[systolic_period] = systolic_peak * (time[systolic_period] / rise_time)
    diastolic_period = np.logical_and(time > rise_time, time <= rise_time + decay_time)
    pulse[diastolic_period] = systolic_peak - (systolic_peak - diastolic_baseline) * ((time[diastolic_period] - rise_time) / decay_time)
    pulse[time > rise_time + decay_time] = diastolic_baseline

    # Dicrotic Notch (with variability)
    notch_amplitude = 0.2 + np.random.normal(0, 0.05)
    notch_time = rise_time + 0.15 + np.random.normal(0, 0.03)
    notch_width = 0.05 + np.random.normal(0, 0.01)
    notch_period = np.logical_and(time >= notch_time - notch_width/2, time <= notch_time + notch_width/2)
    pulse[notch_period] -= notch_amplitude * np.exp(-((time[notch_period] - notch_time) / (notch_width/4))**2)

    # PAD-related Modifications
    if severity > 0:
        # Reduced Amplitude
        amplitude_reduction = 0.2 * severity + np.random.normal(0, 0.01)
        pulse *= (1 - amplitude_reduction)

        # Delayed Peak & Prolonged Decay
        peak_delay = 0.05 * severity + np.random.normal(0, 0.02)
        peak_delay_idx = int((rise_time + peak_delay) * sampling_rate)
        decay_prolongation = 0.1 * severity + np.random.normal(0, 0.03)
        pulse = np.interp(time - peak_delay, time, pulse)
        decay_period = np.logical_and(time > rise_time + peak_delay, time <= rise_time + decay_time + peak_delay + decay_prolongation)
        pulse[decay_period] = pulse[peak_delay_idx] - (pulse[peak_delay_idx] - diastolic_baseline) * ((time[decay_period] - rise_time - peak_delay) / (decay_time + decay_prolongation))

    # Add Noise
    noise_level = 0.05 + np.random.normal(0, 0.01)  # Add variability to noise
    pvr_waveform = pulse + np.random.normal(0, noise_level, len(time))

    return pvr_waveform, time  # Return both waveform and time

# Generate a large dataset
num_samples = 1000
severities = np.random.randint(0, 4, num_samples)  # Random severities
pvr_dataset = [generate_pvr_with_pad(severity, i) for i, severity in enumerate(severities)]
pvr_waveforms, time_arrays = zip(*pvr_dataset)

# Visualize some samples
plt.figure(figsize=(12, 8))
for i in range(5):  # Plot 5 random samples
    plt.plot(time_arrays[i], pvr_waveforms[i], label=f'Severity: {severities[i]}')
plt.xlabel('Time (s)')
plt.ylabel('Amplitude')
plt.title('PVR Waveforms with Varying PAD Severity')
plt.legend()
plt.grid(True)
plt.show()

# Load the dataset
file_path = '/content/test_data.csv'
pvr_data = pd.read_csv(file_path)

# Function to generate synthetic PVR readings by adding noise
def generate_synthetic_pvr_from_waveform(original_waveform, variation_factor, length=400):

    # Generate random noise based on the variation factor
    noise = np.random.normal(0, variation_factor, length)

    # Replicate the original waveform pattern and add noise
    synthetic_waveform = np.tile(original_waveform, length // len(original_waveform) + 1)[:length] + noise

    return synthetic_waveform

# Extract the waveforms for Mild, Critical, Control, and Moderate from the dataset
mild_waveform = pvr_data['RL_PVR_0.82'].values
critical_waveform = pvr_data['RL_PVR_0.65'].values
control_waveform = pvr_data['LL_PVR_1.0'].values
moderate_waveform = pvr_data['LL_PVR_0.68'].values
mild_waveform_1 = pvr_data['LL_PVR_0.82'].values
critical_waveform_1 = pvr_data['RL_PVR_0.44'].values
control_waveform_1 = pvr_data['RL_PVR_1.17'].values
moderate_waveform_1 = pvr_data['RL_PVR_0.71'].values

# Function to generate multiple recordings for each category
def generate_multiple_recordings(n, original_waveform, variation_factor, length=400):
    recordings = []
    for _ in range(n):
        synthetic_waveform = generate_synthetic_pvr_from_waveform(original_waveform, variation_factor, length)
        recordings.append(synthetic_waveform)
    return pd.DataFrame(recordings)

# Generate 50 random recordings for each class
num_recordings = 50

# Length of the synthetic waveform data
waveform_length = 400
variation_factor = 5

# Generate synthetic PVR data for each stage with varying levels of noise/variation
synthetic_pvr_data = {
    'Control': generate_synthetic_pvr_from_waveform(control_waveform, variation_factor=variation_factor, length=waveform_length),
    'Control_1': generate_synthetic_pvr_from_waveform(control_waveform_1, variation_factor=variation_factor, length=waveform_length),
    'Mild': generate_synthetic_pvr_from_waveform(mild_waveform, variation_factor=variation_factor, length=waveform_length),
    'Mild_1': generate_synthetic_pvr_from_waveform(mild_waveform_1, variation_factor=variation_factor, length=waveform_length),
    'Moderate': generate_synthetic_pvr_from_waveform(moderate_waveform, variation_factor=variation_factor, length=waveform_length),
    'Moderate_1': generate_synthetic_pvr_from_waveform(moderate_waveform_1, variation_factor=variation_factor, length=waveform_length),
    'Critical': generate_synthetic_pvr_from_waveform(critical_waveform, variation_factor=variation_factor, length=waveform_length),
    'Critical_1': generate_synthetic_pvr_from_waveform(critical_waveform_1, variation_factor=variation_factor, length=waveform_length),
}

# Generate multiple recordings for each category
control_recordings = generate_multiple_recordings(num_recordings, control_waveform, variation_factor=variation_factor, length=waveform_length)
control_recordings_1 = generate_multiple_recordings(num_recordings, control_waveform_1, variation_factor=variation_factor, length=waveform_length)
mild_recordings = generate_multiple_recordings(num_recordings, mild_waveform, variation_factor=variation_factor, length=waveform_length)
mild_recordings_1 = generate_multiple_recordings(num_recordings, mild_waveform_1, variation_factor=variation_factor, length=waveform_length)
moderate_recordings = generate_multiple_recordings(num_recordings, moderate_waveform, variation_factor=variation_factor, length=waveform_length)
moderate_recordings_1 = generate_multiple_recordings(num_recordings, moderate_waveform_1, variation_factor=variation_factor, length=waveform_length)
critical_recordings = generate_multiple_recordings(num_recordings, critical_waveform, variation_factor=variation_factor, length=waveform_length)
critical_recordings_1 = generate_multiple_recordings(num_recordings, critical_waveform_1, variation_factor=variation_factor, length=waveform_length)

# Add a label column to each dataframe
control_recordings['Label'] = 'Control'
control_recordings_1['Label'] = 'Control'
mild_recordings['Label'] = 'Mild'
mild_recordings_1['Label'] = 'Mild'
moderate_recordings['Label'] = 'Moderate'
moderate_recordings_1['Label'] = 'Moderate'
critical_recordings['Label'] = 'Critical'
critical_recordings_1['Label'] = 'Critical'

# Concatenate all the dataframes to create a larger dataset
full_dataset = pd.concat([control_recordings, mild_recordings, moderate_recordings, critical_recordings], ignore_index=True)

# Display the first few rows of the dataset
print(full_dataset.head())

import matplotlib.pyplot as plt

# Calculate class distribution from full_dataset
class_distribution = full_dataset['Label'].value_counts()

# Plotting the class distribution
plt.figure(figsize=(8, 6))
class_distribution.plot(kind='bar')
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Number of Recordings')
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt
import random

# Function to plot two random recordings from each category
def plot_two_samples_from_each_category(full_dataset):
    # List of unique labels (categories)
    categories = full_dataset['Label'].unique()

    # Create a figure with subplots for each category (2 samples for each category)
    fig, axs = plt.subplots(len(categories), 2, figsize=(15, 12))

    for i, category in enumerate(categories):
        # Filter the data for the current category
        category_data = full_dataset[full_dataset['Label'] == category]

        # Randomly select two samples from the category
        selected_samples = category_data.sample(n=2, random_state=42).drop(columns=['Label'])

        # Plot the two selected samples
        for j, sample in enumerate(selected_samples.values):
            axs[i, j].plot(sample, color='slategray')
            axs[i, j].set_title(f'{category} - Sample {j + 1}')
            axs[i, j].set_xlabel('Time/Index')
            axs[i, j].set_ylabel('Amplitude')

    # Adjust the layout for better visualization
    plt.tight_layout()
    plt.show()

# Plot two samples from each category
plot_two_samples_from_each_category(full_dataset)

import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis

# Function to calculate statistical features for each row (waveform)
def calculate_stats(row):
    stats = {
        'mean': np.mean(row),
        'std': np.std(row),
        'min': np.min(row),
        'max': np.max(row),
        'range': np.max(row) - np.min(row),
        'skewness': skew(row),
        'kurtosis': kurtosis(row)
    }
    return stats

# Function to generate a new DataFrame with statistical features for each waveform
def create_stats_dataframe(full_dataset):
    # Initialize a list to store the features
    feature_list = []

    # Loop through each row of the dataset and calculate statistical features
    for index, row in full_dataset.drop(columns=['Label']).iterrows():
        stats = calculate_stats(row)
        stats['Label'] = full_dataset.iloc[index]['Label']
        feature_list.append(stats)

    # Convert the list of feature dictionaries into a DataFrame
    stats_df = pd.DataFrame(feature_list)

    return stats_df

# Generate the statistical feature DataFrame
stats_df = create_stats_dataframe(full_dataset)

# Display the first few rows of the new DataFrame
print(stats_df.head())

# Save the stats dataframe to a CSV
# stats_df.to_csv('pvr_stats_data.csv', index=False)

from scipy.stats import skew, kurtosis
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

# Function to calculate statistical features for each row (waveform)
def calculate_stats(row):
    stats = {
        'mean': np.mean(row),
        'std': np.std(row),
        'min': np.min(row),
        'max': np.max(row),
        'range': np.max(row) - np.min(row),
        'skewness': skew(row),
        'kurtosis': kurtosis(row)
    }
    return stats

# Function to generate a new DataFrame with statistical features for each waveform
def create_stats_dataframe(full_dataset):
    # Initialize a list to store the features
    feature_list = []

    # Loop through each row of the dataset and calculate statistical features
    for index, row in full_dataset.drop(columns=['Label']).iterrows():
        stats = calculate_stats(row)
        stats['Label'] = full_dataset.iloc[index]['Label']
        feature_list.append(stats)

    # Convert the list of feature dictionaries into a DataFrame
    stats_df = pd.DataFrame(feature_list)

    return stats_df

# Generate the statistical feature DataFrame
stats_df = create_stats_dataframe(full_dataset)

# Split the data into features (X) and labels (y)
X = stats_df.drop(columns=['Label'])
y = stats_df['Label']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Compute the correlation matrix
# Compute the correlation matrix, excluding the 'Label' column (assuming it's your target)
correlation_matrix = stats_df.drop(columns=['Label']).corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Correlation Matrix of Statistical Features')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Load dataset (using Iris dataset as an example)
iris = load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred, normalize='true')

# Create a ConfusionMatrixDisplay object
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)

# Plot the normalized confusion matrix with color scale
disp.plot(cmap=plt.cm.Blues)

# Add title
plt.title('Normalized Confusion Matrix')

# Show the plot
plt.show()

import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler, LabelBinarizer
import matplotlib.pyplot as plt

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features for SVM
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train an SVM model
svm_model = SVC(probability=True, kernel='rbf', random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions
y_pred = svm_model.predict(X_test)
y_pred_proba = svm_model.predict_proba(X_test)

# Compute AU-ROC score
if len(np.unique(y)) == 2:
    auc = roc_auc_score(y_test, y_pred_proba[:, 1])
    print(f'AU-ROC Score: {auc:.2f}')

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])
    plt.plot(fpr, tpr, label=f'AU-ROC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()
else:  # Multi-class case
    lb = LabelBinarizer()
    y_test_binarized = lb.fit_transform(y_test)
    auc = roc_auc_score(y_test_binarized, y_pred_proba, average='macro', multi_class='ovr')
    print(f'Multi-class AU-ROC Score: {auc:.2f}')

# Compute other metrics
accuracy = accuracy_score(y_test, y_pred)

# Get the unique class labels
class_labels = np.unique(y)

# Generate the classification report with per-class metrics
report = classification_report(y_test, y_pred, labels=class_labels)
print(report)

precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

import joblib

# Save the SVM model
joblib.dump(svm_model, '/content/svm_model.joblib')

import numpy as np
import pandas as pd
from scipy.stats import skew, kurtosis
from sklearn.preprocessing import StandardScaler
import joblib

# Load the new dataset
test_data_path = '/content/test_data.csv'
test_df = pd.read_csv(test_data_path, header=None)

# Extract labels from the first row and transpose the data
labels = test_df.iloc[0, :].values
test_df_transposed = test_df.drop(index=0).T

# Add the labels as a new column in the transposed DataFrame
test_df_transposed['Label'] = labels

# Function to calculate statistical features for each row (waveform)
def calculate_stats(row):
    stats = {
        'mean': np.mean(row),
        'std': np.std(row),
        'min': np.min(row),
        'max': np.max(row),
        'range': np.max(row) - np.min(row),
        'skewness': skew(row),
        'kurtosis': kurtosis(row)
    }
    return stats

# Generate the statistical feature DataFrame
def create_stats_dataframe(full_dataset):
    feature_list = []
    for index, row in full_dataset.drop(columns=['Label']).iterrows():
        numeric_row = pd.to_numeric(row, errors='coerce')
        stats = calculate_stats(numeric_row)
        stats['Label'] = full_dataset.iloc[index]['Label']
        feature_list.append(stats)
    stats_df = pd.DataFrame(feature_list)
    return stats_df

# Create the statistical feature DataFrame
test_stats_df = create_stats_dataframe(test_df_transposed)

# Separate features and labels
X_test = test_stats_df.drop(columns=['Label'])
y_test = test_stats_df['Label']

# Load the trained SVM model
svm_model = joblib.load('/content/svm_model.joblib')

# Standardize features
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test)

# Make predictions using the loaded model
y_pred = svm_model.predict(X_test_scaled)

# Print predictions and true labels
test_stats_df['Predicted_Label'] = y_pred
print("Predictions for the test dataset:")
print(test_stats_df[['Label', 'Predicted_Label']])

import pickle

# Save the model to a .pkl file
with open('svm_model.pkl', 'wb') as model_file:
    pickle.dump(svm_model, model_file)

import pickle

# Save the model to a .pkl file
with open('svm_model.pkl', 'wb') as model_file:
    pickle.dump(svm_model, model_file)

with open('scaler.pkl', 'wb') as scaler_file:
    pickle.dump(scaler, scaler_file)