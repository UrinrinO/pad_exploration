# -*- coding: utf-8 -*-
"""Project - ABI Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ipm2niP36waXzjrhosaKpKugvKLSzILm
"""

import pandas as pd

# Read the CSV file into a DataFrame
pd.set_option('display.max_rows', None)
pvr_dataset = pd.read_csv('/content/ABI_PVR_Recordings.csv')

pvr_dataset.head(20)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Data Loading and Selection
df = pd.read_csv('/content/ABI_PVR_Recordings.csv')
selected_column = 'Critical'

# Visualization
plt.figure(figsize=(10, 6))

# Plot original waveform
plt.plot(df[selected_column], label='Original', color='grey', alpha=0.7)

plt.title(f'Waveform for {selected_column} (Normalized & Averaged)')
plt.xlabel('Time/Index')
plt.ylabel('Amplitude')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Data Loading
df = df

# Visualization - Create subplots
fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # 2 rows, 2 columns for 4 subplots
axes = axes.flatten()  # Flatten the 2D array for easier iteration

# Plot each waveform
for i, col in enumerate(df.columns):
    axes[i].plot(df[col], label=(col), color='grey', alpha=0.7)
    axes[i].set_title(f'Waveform for {col}')
    axes[i].set_xlabel('Time/Index')
    axes[i].set_ylabel('Amplitude')
    axes[i].legend()
    axes[i].grid(True)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from scipy import signal

# 1. Apply Butterworth bandpass filter
fs = 100  # Assuming a sampling frequency of 100 Hz (adjust if needed)
lowcut = 0.5  # Adjust lower cutoff frequency as needed
highcut = 20  # Adjust higher cutoff frequency as needed
order = 5  # Adjust filter order as needed

for col in df.columns:
    b, a = signal.butter(order, [lowcut, highcut], btype='band', fs=fs)
    df[col] = signal.filtfilt(b, a, df[col])

# 2. Apply baseline correction
for col in df.columns:
    df[col] = signal.detrend(df[col])

# 3. Plot original and preprocessed signals
fig, axes = plt.subplots(4, 4, figsize=(15, 10))
axes = axes.flatten()

for i, col in enumerate(df.columns):
    axes[i].plot(df.index, df[col], label='Preprocessed', color='red')
    axes[i].set_title(f'Waveform for {col}')
    axes[i].set_xlabel('Time/Index')
    axes[i].set_ylabel('Amplitude')
    axes[i].legend()
    axes[i].grid(True)

# 4. Add titles and labels
fig.suptitle('Original vs. Preprocessed PVR Signals', fontsize=14)
plt.tight_layout()

# 5. Display the plots
plt.show()

# Create a figure with 16 subplots arranged in a 4x4 grid
fig, axes = plt.subplots(4, 4, figsize=(15, 10))

# Iterate through each column of the final_df dataframe
for i, (column_name, column_data) in enumerate(df.items()):
    # Access the corresponding subplot using the index
    row = i // 4
    col = i % 4
    ax = axes[row, col]

    # Determine the color based on the column name (category)
    color = 'red' if column_name == 'Critical' else 'grey'

    # Plot a line chart with the index on the x-axis and the column values on the y-axis
    ax.plot(column_data.index, column_data.values, color=color)

    # Add a title to the subplot indicating the column name (category)
    ax.set_title(column_name)

# Adjust the layout to prevent overlapping elements
plt.tight_layout()

# Display the plot
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from scipy import signal

# 1. Read the CSV file into a DataFrame
df = pd.read_csv('ABI_PVR_Recordings.csv')

# 2. Show the first 5 rows and all columns
print(df.head().to_markdown(index=False, numalign="left", stralign="left"))

# 3. Show columns and their types
print(df.info())

# 4. Create a new figure with a single subplot
plt.figure(figsize=(10, 6))

# 5. Plot the 'ABI 0.82' column
plt.plot(df['Control'], label='Signal Waveform')

# Find peaks and valleys
peaks, _ = signal.find_peaks(df['Control'], prominence=3)
valleys, _ = signal.find_peaks(-df['Control'], prominence=3)  # Invert signal to find valleys

# Plot peaks and valleys
plt.scatter(peaks, df['Control'][peaks], marker='o', color='green', label='Peaks')
plt.scatter(valleys, df['Control'][valleys], marker='o', color='red', label='Valleys')

# Add labels to peaks and valleys
for i, peak in enumerate(peaks):
    plt.annotate(i, (peak, df['Control'][peak]), textcoords="offset points", xytext=(0,10), ha='center')
for i, valley in enumerate(valleys):
    plt.annotate(i, (valley, df['Control'][valley]), textcoords="offset points", xytext=(0,-15), ha='center')

# Add title, labels, and legend
plt.title('Peaks and Valleys of Waveform')
plt.xlabel('Index')
plt.ylabel('Amplitude')
plt.legend()

# Display the plot
plt.show()



import pandas as pd
import matplotlib.pyplot as plt
from scipy import signal

# 1. Apply Butterworth bandpass filter
fs = 100  # Assuming a sampling frequency of 100 Hz
lowcut = 0.5
highcut = 20
order = 5

for col in df.columns:
    b, a = signal.butter(order, [lowcut, highcut], btype='band', fs=fs)
    df[col] = signal.filtfilt(b, a, df[col])

# 2. Apply baseline correction
for col in df.columns:
    df[col] = signal.detrend(df[col])

# 3. Plot the pre-processed waveform signals
fig, axes = plt.subplots(2, 2, figsize=(10, 8))
axes = axes.flatten()

for i, col in enumerate(df.columns):

    # Plot preprocessed waveform in red with solid line
    axes[i].plot(df.index, df[col], label='Preprocessed', color='red', linestyle='--')

    axes[i].set_title(f'Waveform for {col}')
    axes[i].set_xlabel('Time/Index')
    axes[i].set_ylabel('Amplitude')
    axes[i].legend()
    axes[i].grid(True)

# 4. Add titles and labels
fig.suptitle('Preprocessed PVR Signals', fontsize=14)
plt.tight_layout()

# 5. Display the plots
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from scipy import signal

# 1. Apply baseline correction (calculate it first)
baseline_corrected_data = {}
for col in df.columns:
    baseline_corrected_data[col] = signal.detrend(df[col])

# 2. Plot original signals
fig1, axes1 = plt.subplots(2, 2, figsize=(10, 8))
axes1 = axes1.flatten()

for i, col in enumerate(df.columns):
    axes1[i].plot(df.index, df[col], color='gray')
    axes1[i].set_title(f'Original Waveform for {col}')
    axes1[i].set_xlabel('Time/Index')
    axes1[i].set_ylabel('Amplitude')
    axes1[i].grid(True)

fig1.suptitle('Original PVR Signals', fontsize=14)
plt.tight_layout()

# 3. Plot baseline corrected signals
fig2, axes2 = plt.subplots(2, 2, figsize=(10, 8))
axes2 = axes2.flatten()

for i, col in enumerate(df.columns):
    axes2[i].plot(df.index, baseline_corrected_data[col], color='red')
    axes2[i].set_title(f'Baseline Corrected Waveform for {col}')
    axes2[i].set_xlabel('Time/Index')
    axes2[i].set_ylabel('Amplitude')
    axes2[i].grid(True)

fig2.suptitle('Baseline Corrected PVR Signals', fontsize=14)
plt.tight_layout()

# 4. Display the plots
plt.show()

"""# Take Note"""

!pip install tsfresh

from tsfresh.feature_extraction.feature_calculators import extract_features

from scipy.stats import describe

# Extract the first column's name (ABI score) and its values
abi_score = df.columns[0]
pvr_signal = df.iloc[:, 0]

# Compute descriptive statistics
stats_summary = describe(pvr_signal)

# Create a Series from the stats summary and add ABI score
extracted_features_statsmodels = pd.Series(stats_summary)
extracted_features_statsmodels['ABI_score'] = abi_score

# Display the extracted features
print("Extracted Features using Statsmodels for the first person:\n")
print(extracted_features_statsmodels.to_markdown(numalign="left", stralign="left"))

all_extracted_features = []

# Iterate over each column in the DataFrame
for col in df.columns:

    abi_score = col
    pvr_signal = df[col]

    # Compute descriptive statistics
    stats_summary = describe(pvr_signal)

    # Create a Series from the stats summary and add ABI score
    extracted_features_series = pd.Series(stats_summary)
    extracted_features_series['ABI_score'] = abi_score

    # Append the Series to the list
    all_extracted_features.append(extracted_features_series)

# Create a DataFrame from the list of extracted features
extracted_features_df = pd.DataFrame(all_extracted_features)

# Display the first 5 rows of the extracted features DataFrame
print("Extracted Features using Statsmodels for all persons:\n")
print(extracted_features_df.head().to_markdown(numalign="left", stralign="left"))

import numpy as np
import matplotlib.pyplot as plt

def generate_synthetic_pvr(t, amplitude, mean, std_dev, decay_rate):
    """
    Generates a synthetic PVR waveform using a Gaussian and exponential decay model.

    Args:
        t: Time points for the waveform.
        amplitude: Amplitude of the systolic peak.
        mean: Mean (center) of the Gaussian peak.
        std_dev: Standard deviation of the Gaussian peak.
        decay_rate: Decay rate of the exponential function.

    Returns:
        pvr_signal: The generated synthetic PVR waveform.
    """

    # Gaussian component for the systolic peak
    systolic_peak = amplitude * np.exp(-0.5 * ((t - mean) / std_dev)**2)

    # Exponential decay component for the diastolic decline
    diastolic_decline = amplitude * np.exp(-decay_rate * (t - mean)) * (t >= mean)

    # Combine the two components to form the PVR signal
    pvr_signal = systolic_peak + diastolic_decline

    return pvr_signal

# Define parameters for the waveform
t = np.linspace(0, 1, 400)  # 400 time points between 0 and 1 (adjust as needed)
amplitude = 70  # Adjust based on desired range
mean = 0.2
std_dev = 0.05
decay_rate = 5

# Generate the synthetic PVR
synthetic_pvr = generate_synthetic_pvr(t, amplitude, mean, std_dev, decay_rate)

# Plot the synthetic PVR
plt.figure(figsize=(10, 6))
plt.plot(t, synthetic_pvr)
plt.xlabel('Time')
plt.ylabel('PVR Amplitude')
plt.title('Synthetic PVR Waveform')
plt.grid(True)
plt.show()

from tsfresh.feature_extraction.feature_calculators import extract_features

# Extract the first column's name (ABI score) and its values
abi_score = df.columns[0]
pvr_signal = df.iloc[:, 0]

# Create a DataFrame with 'time' and 'pvr' columns
single_person_df = pd.DataFrame({'time': pvr_signal.index, 'pvr': pvr_signal.values})
single_person_df['id'] = 1  # Add an 'id' column for tsfresh

# Extract features using tsfresh
extracted_features = extract_features(single_person_df, column_id='id', column_sort='time')

# Add the ABI score as a new column
extracted_features['ABI_score'] = abi_score

# Display the first 5 rows of the extracted features
print("Extracted Features for the first person:\n")
print(extracted_features.head().to_markdown(numalign="left", stralign="left"))

# Print the shape of extracted_features
print(f"\nShape of extracted_features: {extracted_features.shape}")

import numpy as np
import pandas as pd

# Function to generate synthetic PVR readings
def generate_synthetic_pvr(base_amplitude, variation_factor, frequency_factor, length=400):
    """
    Generates a synthetic waveform with specified base amplitude, variation, and frequency changes.

    Parameters:
    base_amplitude (float): The base amplitude for the waveform
    variation_factor (float): The factor controlling the noise level for variation
    frequency_factor (float): The factor controlling the frequency of oscillation
    length (int): Length of the synthetic waveform to generate

    Returns:
    np.array: A synthetic waveform of the desired length
    """
    # Time points for waveform
    time = np.linspace(0, 10, length)

    # Generate the base waveform with sinusoidal oscillation based on the frequency factor
    base_waveform = base_amplitude * (np.sin(frequency_factor * time) + 1)  # Scale between 0 and 2 * base_amplitude

    # Add random noise to introduce variability
    noise = np.random.normal(0, variation_factor, length)

    # Final waveform by adding noise to the base waveform
    synthetic_waveform = base_waveform + noise
    return synthetic_waveform

# Length of synthetic waveform data
waveform_length = 400

# Generate distinct synthetic PVR data for each PAD stage
synthetic_pvr_data = {
    'Control': generate_synthetic_pvr(base_amplitude=100, variation_factor=5, frequency_factor=1, length=waveform_length),
    'Mild': generate_synthetic_pvr(base_amplitude=90, variation_factor=7, frequency_factor=1.2, length=waveform_length),
    'Moderate': generate_synthetic_pvr(base_amplitude=80, variation_factor=10, frequency_factor=1.5, length=waveform_length),
    'Critical': generate_synthetic_pvr(base_amplitude=70, variation_factor=15, frequency_factor=2, length=waveform_length),
}

# Convert synthetic data into a DataFrame
synthetic_pvr_df = pd.DataFrame(synthetic_pvr_data)

# Display the first few rows of the synthetic data
print(synthetic_pvr_df.head())

# You can also save the dataframe to a CSV if needed
# synthetic_pvr_df.to_csv('distinct_synthetic_pvr_data.csv', index=False)

noise = np.random.normal(0, variation_factor, length)

import numpy as np
import pandas as pd

# Load the dataset
file_path = '/content/ABI_PVR_Recordings.csv'  # Replace with the actual path in Colab
pvr_data = pd.read_csv(file_path)

# Function to generate synthetic PVR readings by adding noise
def generate_synthetic_pvr_from_waveform(original_waveform, variation_factor, length=400):
    """
    Generate synthetic PVR by introducing noise and variability into the provided original waveform.

    Parameters:
    - original_waveform (np.array): The base waveform from which synthetic data is generated
    - variation_factor (float): The factor controlling the level of noise added to the waveform
    - length (int): The number of points in the generated waveform

    Returns:
    - np.array: Synthetic PVR waveform
    """
    # Generate random noise based on the variation factor
    noise = np.random.normal(0, variation_factor, length)

    # Replicate the original waveform pattern and add noise
    synthetic_waveform = np.tile(original_waveform, length // len(original_waveform) + 1)[:length] + noise

    return synthetic_waveform

# Extract the waveforms for Mild, Critical, Control, and Moderate from the dataset
mild_waveform = pvr_data['Mild'].values
critical_waveform = pvr_data['Critical'].values
control_waveform = pvr_data['Control'].values
moderate_waveform = pvr_data['Moderate'].values

# Length of the synthetic waveform data
waveform_length = 400
variation_factor = 10

# Generate synthetic PVR data for each stage with varying levels of noise/variation
synthetic_pvr_data = {
    'Control': generate_synthetic_pvr_from_waveform(control_waveform, variation_factor=variation_factor, length=waveform_length),
    'Mild': generate_synthetic_pvr_from_waveform(mild_waveform, variation_factor=variation_factor, length=waveform_length),
    'Moderate': generate_synthetic_pvr_from_waveform(moderate_waveform, variation_factor=variation_factor, length=waveform_length),
    'Critical': generate_synthetic_pvr_from_waveform(critical_waveform, variation_factor=variation_factor, length=waveform_length),
}

# Convert the synthetic data into a DataFrame
synthetic_pvr_df = pd.DataFrame(synthetic_pvr_data)

# Display the first few rows of the DataFrame
print(synthetic_pvr_df.head())

# Optionally, save the dataframe to a CSV file for future use
# synthetic_pvr_df.to_csv('synthetic_pvr_data.csv', index=False)

import numpy as np
import pandas as pd

# Load the dataset
file_path = '/content/ABI_PVR_Recordings.csv'
pvr_data = pd.read_csv(file_path)

# Function to generate synthetic PVR readings by adding noise
def generate_synthetic_pvr_from_waveform(original_waveform, variation_factor, length=400):

    # Generate random noise based on the variation factor
    noise = np.random.normal(0, variation_factor, length)

    # Replicate the original waveform pattern and add noise
    synthetic_waveform = np.tile(original_waveform, length // len(original_waveform) + 1)[:length] + noise

    return synthetic_waveform

# Extract the waveforms for Mild, Critical, Control, and Moderate from the dataset
mild_waveform = pvr_data['Mild'].values
critical_waveform = pvr_data['Critical'].values
control_waveform = pvr_data['Control'].values
moderate_waveform = pvr_data['Moderate'].values

# Function to generate multiple recordings for each category
def generate_multiple_recordings(n, original_waveform, variation_factor, length=400):
    recordings = []
    for _ in range(n):
        synthetic_waveform = generate_synthetic_pvr_from_waveform(original_waveform, variation_factor, length)
        recordings.append(synthetic_waveform)
    return pd.DataFrame(recordings)

# Generate 50 random recordings for each class
num_recordings = 50

# Length of the synthetic waveform data
waveform_length = 400
variation_factor = 10

# Generate synthetic PVR data for each stage with varying levels of noise/variation
synthetic_pvr_data = {
    'Control': generate_synthetic_pvr_from_waveform(control_waveform, variation_factor=variation_factor, length=waveform_length),
    'Mild': generate_synthetic_pvr_from_waveform(mild_waveform, variation_factor=variation_factor, length=waveform_length),
    'Moderate': generate_synthetic_pvr_from_waveform(moderate_waveform, variation_factor=variation_factor, length=waveform_length),
    'Critical': generate_synthetic_pvr_from_waveform(critical_waveform, variation_factor=variation_factor, length=waveform_length),
}

# Generate multiple recordings for each category
control_recordings = generate_multiple_recordings(num_recordings, control_waveform, variation_factor=10, length=waveform_length)
mild_recordings = generate_multiple_recordings(num_recordings, mild_waveform, variation_factor=8, length=waveform_length)
moderate_recordings = generate_multiple_recordings(num_recordings, moderate_waveform, variation_factor=3, length=waveform_length)
critical_recordings = generate_multiple_recordings(num_recordings, critical_waveform, variation_factor=2, length=waveform_length)

# Add a label column to each dataframe
control_recordings['Label'] = 'Control'
mild_recordings['Label'] = 'Mild'
moderate_recordings['Label'] = 'Moderate'
critical_recordings['Label'] = 'Critical'

# Concatenate all the dataframes to create a larger dataset
full_dataset = pd.concat([control_recordings, mild_recordings, moderate_recordings, critical_recordings], ignore_index=True)

# Display the first few rows of the dataset
print(full_dataset.head())

full_dataset_T = full_dataset.transpose()

full_dataset_T.head()

import matplotlib.pyplot as plt
import random

# Function to plot two random recordings from each category
def plot_two_samples_from_each_category(full_dataset):
    # List of unique labels (categories)
    categories = full_dataset['Label'].unique()

    # Create a figure with subplots for each category (2 samples for each category)
    fig, axs = plt.subplots(len(categories), 2, figsize=(15, 12))

    for i, category in enumerate(categories):
        # Filter the data for the current category
        category_data = full_dataset[full_dataset['Label'] == category]

        # Randomly select two samples from the category
        selected_samples = category_data.sample(n=2, random_state=42).drop(columns=['Label'])

        # Plot the two selected samples
        for j, sample in enumerate(selected_samples.values):
            axs[i, j].plot(sample, color='slategray')
            axs[i, j].set_title(f'{category} - Sample {j + 1}')
            axs[i, j].set_xlabel('Time/Index')
            axs[i, j].set_ylabel('Amplitude')

    # Adjust the layout for better visualization
    plt.tight_layout()
    plt.show()

# Plot two samples from each category
plot_two_samples_from_each_category(full_dataset)

import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis

# Function to calculate statistical features for each row (waveform)
def calculate_stats(row):
    """
    Calculate statistical features for a given waveform (row).

    Parameters:
    - row (pd.Series): A single waveform (row) of the dataset.

    Returns:
    - dict: A dictionary of computed statistical features.
    """
    stats = {
        'mean': np.mean(row),
        'std': np.std(row),
        'min': np.min(row),
        'max': np.max(row),
        'range': np.max(row) - np.min(row),
        'skewness': skew(row),
        'kurtosis': kurtosis(row)
    }
    return stats

# Function to generate a new DataFrame with statistical features for each waveform
def create_stats_dataframe(full_dataset):
    # Initialize a list to store the features
    feature_list = []

    # Loop through each row of the dataset and calculate statistical features
    for index, row in full_dataset.drop(columns=['Label']).iterrows():
        stats = calculate_stats(row)
        stats['Label'] = full_dataset.iloc[index]['Label']  # Add the target label (Control, Mild, etc.)
        feature_list.append(stats)

    # Convert the list of feature dictionaries into a DataFrame
    stats_df = pd.DataFrame(feature_list)

    return stats_df

# Generate the statistical feature DataFrame
stats_df = create_stats_dataframe(full_dataset)

# Display the first few rows of the new DataFrame
print(stats_df.head())

# Optionally, save the stats dataframe to a CSV
# stats_df.to_csv('pvr_stats_data.csv', index=False)

import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Function to calculate statistical features for each row (waveform)
def calculate_stats(row):
    """
    Calculate statistical features for a given waveform (row).

    Parameters:
    - row (pd.Series): A single waveform (row) of the dataset.

    Returns:
    - dict: A dictionary of computed statistical features.
    """
    stats = {
        'mean': np.mean(row),
        'std': np.std(row),
        'min': np.min(row),
        'max': np.max(row),
        'range': np.max(row) - np.min(row),
        'skewness': skew(row),
        'kurtosis': kurtosis(row)
    }
    return stats

# Function to generate a new DataFrame with statistical features for each waveform
def create_stats_dataframe(full_dataset):
    # Initialize a list to store the features
    feature_list = []

    # Loop through each row of the dataset and calculate statistical features
    for index, row in full_dataset.drop(columns=['Label']).iterrows():
        stats = calculate_stats(row)
        stats['Label'] = full_dataset.iloc[index]['Label']  # Add the target label (Control, Mild, etc.)
        feature_list.append(stats)

    # Convert the list of feature dictionaries into a DataFrame
    stats_df = pd.DataFrame(feature_list)

    return stats_df

# Assume you already have 'full_dataset' from earlier steps (waveforms + Label column)

# Step 1: Generate the statistical feature DataFrame
stats_df = create_stats_dataframe(full_dataset)

# Step 2: Split the data into features (X) and labels (y)
X = stats_df.drop(columns=['Label'])  # Features (all columns except Label)
y = stats_df['Label']  # Labels (target)

# Step 3: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 4: Train a Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Step 5: Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Step 6: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Limit the complexity of the Random Forest to avoid overfitting
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,              # Limit the depth of the trees
    min_samples_split=10,     # Minimum number of samples required to split a node
    min_samples_leaf=5,       # Minimum number of samples required to be at a leaf node
    random_state=42
)

# Cross-validation with 5 folds
cv_scores = cross_val_score(rf_model, X, y, cv=5)

# Print cross-validation scores
print(f"Cross-validation accuracy scores: {cv_scores}")
print(f"Mean CV Accuracy: {cv_scores.mean() * 100:.2f}%")

# Train-test split to evaluate the model on separate test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the Random Forest model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

# Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Load dataset (using Iris dataset as an example)
iris = load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Generate confusion matrix
cm = confusion_matrix(y_test, y_pred, normalize='true')  # Normalized confusion matrix

# Create a ConfusionMatrixDisplay object
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)

# Plot the normalized confusion matrix with color scale
disp.plot(cmap=plt.cm.Blues)

# Add title
plt.title('Normalized Confusion Matrix')

# Show the plot
plt.show()

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelBinarizer

# Assuming `X` is your feature DataFrame and `y` is your target labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)
y_pred_proba = rf_model.predict_proba(X_test)  # Predicted probabilities for AU-ROC calculation

# Binary classification or multi-class AU-ROC computation
if len(np.unique(y)) == 2:  # Binary classification case
    # Compute AU-ROC score for binary classification
    auc = roc_auc_score(y_test, y_pred_proba[:, 1])  # Use probability of the positive class
    print(f'AU-ROC Score: {auc:.2f}')

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])
    plt.plot(fpr, tpr, label=f'AU-ROC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Random classifier line
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()
else:  # Multi-class case
    # Binarize the output labels for multi-class AU-ROC calculation
    lb = LabelBinarizer()
    y_test_binarized = lb.fit_transform(y_test)
    auc = roc_auc_score(y_test_binarized, y_pred_proba, average='macro', multi_class='ovr')
    print(f'Multi-class AU-ROC Score: {auc:.2f}')

# Compute other metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler, LabelBinarizer
import matplotlib.pyplot as plt

# Assuming X (features) and y (target labels) are already defined
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features for SVM
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train an SVM model
svm_model = SVC(probability=True, kernel='rbf', random_state=42)  # Using RBF kernel here
svm_model.fit(X_train, y_train)

# Make predictions
y_pred = svm_model.predict(X_test)
y_pred_proba = svm_model.predict_proba(X_test)  # To get probabilities for AU-ROC

# Compute AU-ROC score
if len(np.unique(y)) == 2:  # Binary classification case
    auc = roc_auc_score(y_test, y_pred_proba[:, 1])  # Use probability of the positive class
    print(f'AU-ROC Score: {auc:.2f}')

    # Plot ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])
    plt.plot(fpr, tpr, label=f'AU-ROC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Random classifier line
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()
else:  # Multi-class case
    lb = LabelBinarizer()
    y_test_binarized = lb.fit_transform(y_test)
    auc = roc_auc_score(y_test_binarized, y_pred_proba, average='macro', multi_class='ovr')
    print(f'Multi-class AU-ROC Score: {auc:.2f}')

# Compute other metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X (features) and y (target labels) are already defined

# If X is a numpy array, convert it to a DataFrame
# Add column names if they are known (or use default names)
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]  # Replace with actual names if available
X_df = pd.DataFrame(X, columns=feature_names)

# Step 1: Compute the correlation matrix for the feature set X
correlation_matrix = X_df.corr()

# Step 2: Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Feature Correlation Matrix')
plt.show()

# Optional Step: Drop highly correlated features (if any) based on a threshold
correlation_threshold = 0.9

upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
columns_to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column].abs() > correlation_threshold)]

print(f"Columns to drop due to high correlation (>|{correlation_threshold}|): {columns_to_drop}")

# Drop the highly correlated columns (if any)
X_reduced = X_df.drop(columns=columns_to_drop)

# Step 3: Split the (possibly reduced) data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=42)

# Step 4: Train a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Step 5: Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Step 6: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')